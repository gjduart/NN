{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Notebook - Lab 01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gjduart/NN/blob/main/Python_Notebook_Lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN5xQzblg0JQ",
        "outputId": "7d8ce633-53db-4127-c197-5e3b8100c57a"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = (iris.target != 0).astype(int) # Setosas vs Versicolor & Virginica\n",
        "\n",
        "\n",
        "# Create test, validation and train sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "     X_train, y_train, test_size=0.20, random_state=1)\n",
        "\n",
        "\n",
        "# Create a network with 1 linear unit\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(units=1, input_shape=[4], activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='SGD',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        "  )\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model (and perform validation on it)\n",
        "history = model.fit(X_train, y_train, \n",
        "                  validation_data=(X_val, y_val),\n",
        "                  epochs=100, \n",
        "                  verbose=False)\n",
        "\n",
        "# Simlar to .score method in Scikit-learn\n",
        "results = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Prediction sample-by-sample\n",
        "print('\\n\\nChecking the results on test set:')\n",
        "for x, y in zip(X_test, y_test):\n",
        "  \n",
        "  o = model.predict(np.reshape(x, (1, -1)))\n",
        "  \n",
        "  o = np.round(o).item()\n",
        "  \n",
        "  print('The model output is {} when it should be {}.'.format(o, y))\n",
        "\n",
        "# Overall results\n",
        "print('\\nChecking the results (a quick resume)')\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2041 - binary_accuracy: 1.0000\n",
            "\n",
            "\n",
            "Checking the results on test set:\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 1.0 when it should be 1.\n",
            "The model output is 0.0 when it should be 0.\n",
            "The model output is 0.0 when it should be 0.\n",
            "\n",
            "Checking the results (a quick resume)\n",
            "loss: 0.204\n",
            "binary_accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaQp-Sj_JO8t"
      },
      "source": [
        "# Conserte o modelo acima para conseguir uma acurácia de 100% (esse problema é mamão com áçucar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOAVeZrrJOWC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5iRbD3uDz79"
      },
      "source": [
        "# Análise de risco de crédito na Alemanha (pré-Euro).\n",
        "\n",
        "Quando um banco recebe um pedido de empréstimo, com base no perfil do solicitante, o banco deve tomar uma decisão sobre a aprovação ou não aprovação do empréstimo. Dessa perspectiva, dois tipos de riscos estão associados à decisão do banco:\n",
        "\n",
        "1. Se o requerente tiver um bom risco de crédito, ou seja, for provável que reembolse o empréstimo, então, não aprovar o empréstimo a esse requerente resulta numa perda de negócios para o banco;\n",
        "2. Por outro lado, se o requerente for um risco de crédito desfavorável, ou seja, não é provável que reembolse o empréstimo, a aprovação do empréstimo a esse requerente resulta numa perda financeira para o banco.\n",
        "\n",
        "Para minimizar a perda do ponto de vista do banco, é necessária uma regra de decisão com relação a quem conceder a aprovação do empréstimo e quem não deve. Os perfis demográficos e socioeconômicos do candidato são considerados pelos gerentes de empréstimo antes de tomarem essa decisão. Espera-se que um modelo preditivo treinado com esses dados forneça uma orientação para o gerente de empréstimos no processo de tomada de decisão, tudo com base nos perfis do candidato.\n",
        "Para ilustrar tal cenário, adotamos uma versão *nova* do conjunto de dados de crédito alemão contendo **apenas** 9 variáveis e os respectivos rótulos de classificação indicando se um candidato é considerado um risco de crédito Bom ou Ruim. \n",
        "\n",
        "As informações de cada candidato é descrita da seguinte forma:\n",
        "\n",
        "```\n",
        "1. Idade (numérica);\n",
        "2. Sexo (texto: masculino, feminino);\n",
        "3. Emprego (numérico: 0 - não qualificado e não residente, 1 - não qualificado e residente, 2 - habilitado, 3 - altamente qualificado);\n",
        "4. Habitação (texto: próprio, alugado ou gratuito);\n",
        "5. Contas de poupança (texto - pequeno, moderado, bastante rico, rico);\n",
        "6. Conta corrente (numérica, em DM - Marco Alemão);\n",
        "7. Valor do crédito (numérico, em DM);\n",
        "8. Duração (numérica, no mês);\n",
        "9. Propósito (texto: carro, móveis / equipamentos, rádio / TV, eletrodomésticos, reparos, educação, negócios, férias / outros).\n",
        "10. Rótulo (+1 empréstimo aceito, -1 negado).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvH0qH64g5Vr",
        "outputId": "107d7916-07e4-480d-ea61-9f8d381adecc"
      },
      "source": [
        "# Download the Statlog (German Credit Data) Data Set  from UCI Machine Learning\n",
        "# (Só que do meu Github)\n",
        "!wget 'https://github.com/sauloafoliveira/huawei-labs/raw/master/german_credit_train.txt' -O german_score_credit_train.csv\n",
        "!wget 'https://github.com/sauloafoliveira/huawei-labs/raw/master/german_credit_test.txt' -O german_score_credit_test.csv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-08 04:10:29--  https://github.com/sauloafoliveira/huawei-labs/raw/master/german_credit_train.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sauloafoliveira/huawei-labs/master/german_credit_train.txt [following]\n",
            "--2021-03-08 04:10:29--  https://raw.githubusercontent.com/sauloafoliveira/huawei-labs/master/german_credit_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33186 (32K) [text/plain]\n",
            "Saving to: ‘german_score_credit_train.csv’\n",
            "\n",
            "german_score_credit 100%[===================>]  32.41K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-03-08 04:10:29 (8.26 MB/s) - ‘german_score_credit_train.csv’ saved [33186/33186]\n",
            "\n",
            "--2021-03-08 04:10:29--  https://github.com/sauloafoliveira/huawei-labs/raw/master/german_credit_test.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sauloafoliveira/huawei-labs/master/german_credit_test.txt [following]\n",
            "--2021-03-08 04:10:29--  https://raw.githubusercontent.com/sauloafoliveira/huawei-labs/master/german_credit_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14185 (14K) [text/plain]\n",
            "Saving to: ‘german_score_credit_test.csv’\n",
            "\n",
            "german_score_credit 100%[===================>]  13.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-08 04:10:29 (78.4 MB/s) - ‘german_score_credit_test.csv’ saved [14185/14185]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWhDgFWrxHQ6"
      },
      "source": [
        "Criem uma rede MLP decente (usando o TF) que seja capaz de obter algo em torno de 60-70% de taxa de acurácia. Use o arquivo que termina com ```german_score_credit_train.csv``` para fazer o treinamento e o  ```german_score_credit_test.csv``` para calcularem a taxa de acurácia como visto no exemplo acima.\n",
        "\n",
        "\n",
        "> Por decente, eu quis dizer um modelo que ao multiplicar o número de camadas ocultas pelo número de neurônios não ultrapassem 50! Esse é o desafio.\n",
        "\n",
        "> Não esqueçam de todo o framework que vocês viram nas aulas passadas: pré-processamento, normalização, etc.\n",
        "\n",
        "> Mostrem também o histograma da saída das predições de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k0FQ1F74c4h"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "german_score_train = pd.read_csv('german_score_credit_train.csv', delimiter=\"\\t\" )\r\n",
        "german_score_test  = pd.read_csv('german_score_credit_test.csv', delimiter=\"\\t\")\r\n",
        "#german_score_train = german_score_train.convert_dtypes()\r\n",
        "german_score_train['female'] = pd.Categorical(german_score_train['female'])\r\n",
        "german_score_train['female'] = german_score_train.female.cat.codes\r\n",
        "\r\n",
        "german_score_train['own'] = pd.Categorical(german_score_train['own'])\r\n",
        "german_score_train['own'] = german_score_train.own.cat.codes\r\n",
        "\r\n",
        "german_score_train['little'] = pd.Categorical(german_score_train['little'])\r\n",
        "german_score_train['little'] = german_score_train.little.cat.codes\r\n",
        "\r\n",
        "german_score_train['no'] = pd.Categorical(german_score_train['no'])\r\n",
        "german_score_train['no'] = german_score_train.no.cat.codes\r\n",
        "\r\n",
        "german_score_train['car'] = pd.Categorical(german_score_train['car'])\r\n",
        "german_score_train['car'] = german_score_train.car.cat.codes\r\n",
        "\r\n",
        "german_score_test['female'] = pd.Categorical(german_score_test['female'])\r\n",
        "german_score_test['female'] = german_score_test.female.cat.codes\r\n",
        "\r\n",
        "german_score_test['own'] = pd.Categorical(german_score_test['own'])\r\n",
        "german_score_test['own'] = german_score_test.own.cat.codes\r\n",
        "\r\n",
        "german_score_test['little'] = pd.Categorical(german_score_test['little'])\r\n",
        "german_score_test['little'] = german_score_test.little.cat.codes\r\n",
        "\r\n",
        "german_score_test['moderate'] = pd.Categorical(german_score_test['moderate'])\r\n",
        "german_score_test['moderate'] = german_score_test.moderate.cat.codes\r\n",
        "\r\n",
        "german_score_test['business'] = pd.Categorical(german_score_test['business'])\r\n",
        "german_score_test['business'] = german_score_test.business.cat.codes\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4SOvZJP9wn9"
      },
      "source": [
        "def normalize(df):\r\n",
        "    result = df.copy()\r\n",
        "    max_value = df.max()\r\n",
        "    min_value = df.min()\r\n",
        "    result = (df - min_value) / (max_value - min_value)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CGpwD1e-bnTd",
        "outputId": "476f7c2b-13b0-43f7-a925-33d74ab23012"
      },
      "source": [
        "german_score_train  = normalize(german_score_train)\r\n",
        "german_score_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>38</th>\n",
              "      <th>female</th>\n",
              "      <th>0</th>\n",
              "      <th>own</th>\n",
              "      <th>little</th>\n",
              "      <th>no</th>\n",
              "      <th>926</th>\n",
              "      <th>12</th>\n",
              "      <th>car</th>\n",
              "      <th>-1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.071429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.787113</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.058380</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.018708</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.071429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.848575</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.178571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.090844</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.328986</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0.196429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.204083</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.758061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>0.428571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.145758</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>0.214286</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>699 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           38  female         0  own  ...       926        12       car   -1\n",
              "0    0.071429     1.0  0.000000  0.5  ...  0.787113  0.035714  0.142857  1.0\n",
              "1    0.125000     1.0  0.666667  0.5  ...  0.058380  0.357143  0.714286  0.0\n",
              "2    0.125000     1.0  0.333333  0.5  ...  0.018708  0.035714  0.714286  0.0\n",
              "3    0.071429     1.0  0.666667  0.5  ...  0.848575  0.785714  0.000000  1.0\n",
              "4    0.178571     1.0  1.000000  1.0  ...  0.090844  0.357143  0.714286  0.0\n",
              "..        ...     ...       ...  ...  ...       ...       ...       ...  ...\n",
              "694  0.071429     0.0  0.333333  1.0  ...  0.328986  0.571429  0.571429  1.0\n",
              "695  0.196429     1.0  1.000000  0.5  ...  0.204083  0.571429  0.571429  0.0\n",
              "696  0.142857     1.0  1.000000  0.5  ...  0.758061  1.000000  0.142857  1.0\n",
              "697  0.428571     1.0  0.666667  0.5  ...  0.145758  0.250000  0.142857  0.0\n",
              "698  0.214286     1.0  0.666667  0.5  ...  0.172114  0.250000  0.142857  0.0\n",
              "\n",
              "[699 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "W-z2147n-9Rz",
        "outputId": "f22b4778-70f8-4263-bd4c-c75fbc28a2eb"
      },
      "source": [
        "german_score_test  = normalize(german_score_test)\r\n",
        "german_score_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>39</th>\n",
              "      <th>female</th>\n",
              "      <th>2</th>\n",
              "      <th>own</th>\n",
              "      <th>little</th>\n",
              "      <th>moderate</th>\n",
              "      <th>1188</th>\n",
              "      <th>21</th>\n",
              "      <th>business</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.306547</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.036364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.050715</td>\n",
              "      <td>0.205882</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.163636</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.046978</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.176440</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.309091</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.118701</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0.381818</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.226704</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>0.036364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.035249</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0.363636</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.641706</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.141706</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>0.145455</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.385037</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>298 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           39  female         2  own  ...      1188        21  business    1\n",
              "0    0.200000     0.0  1.000000  0.5  ...  0.306547  0.647059  0.571429  1.0\n",
              "1    0.036364     0.0  0.666667  1.0  ...  0.050715  0.205882  0.714286  0.0\n",
              "2    0.163636     1.0  0.666667  0.5  ...  0.046978  0.029412  0.714286  0.0\n",
              "3    0.600000     1.0  0.666667  0.5  ...  0.176440  0.117647  0.714286  0.0\n",
              "4    0.309091     1.0  0.666667  0.5  ...  0.118701  0.382353  0.142857  0.0\n",
              "..        ...     ...       ...  ...  ...       ...       ...       ...  ...\n",
              "293  0.381818     1.0  1.000000  0.5  ...  0.226704  0.382353  0.142857  0.0\n",
              "294  0.036364     0.0  0.666667  0.5  ...  0.035249  0.117647  0.714286  0.0\n",
              "295  0.363636     1.0  0.666667  0.0  ...  0.641706  0.647059  0.142857  1.0\n",
              "296  0.509091     1.0  0.333333  0.5  ...  0.141706  0.294118  0.142857  1.0\n",
              "297  0.145455     1.0  1.000000  0.5  ...  0.385037  0.294118  1.000000  0.0\n",
              "\n",
              "[298 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFWyGRXeZ8Kh",
        "outputId": "d0e0473c-d9f7-4bc2-d799-f6b42494ec05"
      },
      "source": [
        "Y_train = german_score_train.pop('-1')\r\n",
        "X_train = german_score_train\r\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(699, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDUh5SyHg6Rj"
      },
      "source": [
        "n_validation = int(X_train.shape[0]*0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhnHU45cJXp"
      },
      "source": [
        "\r\n",
        "Y_val = Y_train[:n_validation]\r\n",
        "X_val = X_train[:n_validation]\r\n",
        "\r\n",
        "X_train = X_train[n_validation:]\r\n",
        "Y_train = Y_train[n_validation:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH4gVpS9fTk2",
        "outputId": "fe89d12f-d5f9-4cce-8c88-f12be32ee9c3"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(525, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt-ts1-gFBdC"
      },
      "source": [
        "\r\n",
        "Y_test = german_score_test.pop('1')\r\n",
        "X_test = german_score_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHoMFC9ihvq",
        "outputId": "fc619437-2ff0-4101-fc2b-05580a69abdc"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(298, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5X2J03ZQkW5"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras import regularizers\r\n",
        "from keras import optimizers\r\n",
        "from keras.layers import Dense, Dropout\r\n",
        "sgd = optimizers.SGD(lr=1e-3, decay=1e-5, momentum=0.9, nesterov=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzNfgmYlOBPJ"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(units=20, activation='tanh', input_dim=9, kernel_initializer='glorot_normal', bias_initializer='zeros'))#, kernel_regularizer=regularizers.l2(0.01)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(units=1, activation='relu', bias_initializer='zeros'))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5326uG-J9W8c",
        "outputId": "560edc83-a49e-4470-f8e0-826fb061b947"
      },
      "source": [
        "\r\n",
        "model.compile(\r\n",
        "    optimizer=sgd,\r\n",
        "    loss=tf.keras.losses.binary_crossentropy,\r\n",
        "    metrics=['accuracy'],\r\n",
        "  )\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "\r\n",
        "history = model.fit(X_train.values, Y_train.values, \r\n",
        "                  validation_data=(X_val.values, Y_val.values),\r\n",
        "                  epochs=300, batch_size=128,\r\n",
        "                  verbose=True)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 20)                200       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "5/5 [==============================] - 1s 47ms/step - loss: 2.2854 - accuracy: 0.4504 - val_loss: 0.8316 - val_accuracy: 0.5862\n",
            "Epoch 2/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.3702 - accuracy: 0.5940 - val_loss: 1.1986 - val_accuracy: 0.6897\n",
            "Epoch 3/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.6456 - accuracy: 0.6648 - val_loss: 0.8838 - val_accuracy: 0.6954\n",
            "Epoch 4/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.4917 - accuracy: 0.6249 - val_loss: 0.7532 - val_accuracy: 0.6839\n",
            "Epoch 5/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 1.1347 - accuracy: 0.5941 - val_loss: 0.6794 - val_accuracy: 0.7011\n",
            "Epoch 6/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.2023 - accuracy: 0.5934 - val_loss: 0.6593 - val_accuracy: 0.7011\n",
            "Epoch 7/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.1156 - accuracy: 0.6072 - val_loss: 0.6630 - val_accuracy: 0.7011\n",
            "Epoch 8/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 1.2358 - accuracy: 0.6658 - val_loss: 0.7061 - val_accuracy: 0.7011\n",
            "Epoch 9/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 1.4277 - accuracy: 0.6610 - val_loss: 0.6370 - val_accuracy: 0.7011\n",
            "Epoch 10/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.8827 - accuracy: 0.6724 - val_loss: 0.6322 - val_accuracy: 0.7011\n",
            "Epoch 11/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7981 - accuracy: 0.6864 - val_loss: 0.6883 - val_accuracy: 0.7069\n",
            "Epoch 12/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.9531 - accuracy: 0.6466 - val_loss: 0.6822 - val_accuracy: 0.7069\n",
            "Epoch 13/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.8970 - accuracy: 0.6977 - val_loss: 0.6180 - val_accuracy: 0.7011\n",
            "Epoch 14/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.8309 - accuracy: 0.6864 - val_loss: 0.6178 - val_accuracy: 0.6954\n",
            "Epoch 15/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7538 - accuracy: 0.6810 - val_loss: 0.6223 - val_accuracy: 0.6897\n",
            "Epoch 16/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 1.0045 - accuracy: 0.6274 - val_loss: 0.6225 - val_accuracy: 0.6897\n",
            "Epoch 17/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7842 - accuracy: 0.6563 - val_loss: 0.5989 - val_accuracy: 0.7011\n",
            "Epoch 18/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6346 - accuracy: 0.6814 - val_loss: 0.5907 - val_accuracy: 0.7011\n",
            "Epoch 19/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7597 - accuracy: 0.6969 - val_loss: 0.6781 - val_accuracy: 0.6954\n",
            "Epoch 20/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.9194 - accuracy: 0.7271 - val_loss: 0.6714 - val_accuracy: 0.7011\n",
            "Epoch 21/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7672 - accuracy: 0.7320 - val_loss: 0.5939 - val_accuracy: 0.7069\n",
            "Epoch 22/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7164 - accuracy: 0.7344 - val_loss: 0.5878 - val_accuracy: 0.6839\n",
            "Epoch 23/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7358 - accuracy: 0.6804 - val_loss: 0.5851 - val_accuracy: 0.7126\n",
            "Epoch 24/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6933 - accuracy: 0.6553 - val_loss: 0.5856 - val_accuracy: 0.7241\n",
            "Epoch 25/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6913 - accuracy: 0.6992 - val_loss: 0.5851 - val_accuracy: 0.7069\n",
            "Epoch 26/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7078 - accuracy: 0.6702 - val_loss: 0.5863 - val_accuracy: 0.6954\n",
            "Epoch 27/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.8214 - accuracy: 0.7038 - val_loss: 0.6487 - val_accuracy: 0.6954\n",
            "Epoch 28/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7525 - accuracy: 0.7189 - val_loss: 0.6483 - val_accuracy: 0.6954\n",
            "Epoch 29/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6413 - accuracy: 0.6987 - val_loss: 0.6082 - val_accuracy: 0.7126\n",
            "Epoch 30/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6582 - accuracy: 0.6875 - val_loss: 0.6200 - val_accuracy: 0.7011\n",
            "Epoch 31/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6297 - accuracy: 0.6825 - val_loss: 0.6051 - val_accuracy: 0.7069\n",
            "Epoch 32/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6202 - accuracy: 0.7024 - val_loss: 0.5938 - val_accuracy: 0.7126\n",
            "Epoch 33/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7056 - accuracy: 0.7053 - val_loss: 0.6031 - val_accuracy: 0.7011\n",
            "Epoch 34/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.8837 - accuracy: 0.6898 - val_loss: 0.6107 - val_accuracy: 0.7011\n",
            "Epoch 35/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7344 - accuracy: 0.7274 - val_loss: 0.6003 - val_accuracy: 0.7126\n",
            "Epoch 36/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6076 - accuracy: 0.6769 - val_loss: 0.6072 - val_accuracy: 0.6897\n",
            "Epoch 37/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6245 - accuracy: 0.6826 - val_loss: 0.6121 - val_accuracy: 0.6897\n",
            "Epoch 38/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6239 - accuracy: 0.7047 - val_loss: 0.6080 - val_accuracy: 0.6954\n",
            "Epoch 39/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6337 - accuracy: 0.7021 - val_loss: 0.6826 - val_accuracy: 0.6954\n",
            "Epoch 40/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6874 - accuracy: 0.7382 - val_loss: 0.7453 - val_accuracy: 0.6954\n",
            "Epoch 41/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6658 - accuracy: 0.7429 - val_loss: 0.6817 - val_accuracy: 0.7011\n",
            "Epoch 42/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5862 - accuracy: 0.7520 - val_loss: 0.6069 - val_accuracy: 0.6897\n",
            "Epoch 43/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6427 - accuracy: 0.7114 - val_loss: 0.6110 - val_accuracy: 0.7069\n",
            "Epoch 44/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5839 - accuracy: 0.6971 - val_loss: 0.6016 - val_accuracy: 0.6897\n",
            "Epoch 45/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5790 - accuracy: 0.7263 - val_loss: 0.5951 - val_accuracy: 0.6954\n",
            "Epoch 46/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6378 - accuracy: 0.7347 - val_loss: 0.6614 - val_accuracy: 0.6954\n",
            "Epoch 47/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6357 - accuracy: 0.7509 - val_loss: 0.6596 - val_accuracy: 0.6897\n",
            "Epoch 48/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6423 - accuracy: 0.7124 - val_loss: 0.6559 - val_accuracy: 0.6897\n",
            "Epoch 49/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6661 - accuracy: 0.7330 - val_loss: 0.6529 - val_accuracy: 0.6897\n",
            "Epoch 50/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6765 - accuracy: 0.7231 - val_loss: 0.6518 - val_accuracy: 0.6897\n",
            "Epoch 51/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.7585 - accuracy: 0.7163 - val_loss: 0.6499 - val_accuracy: 0.6954\n",
            "Epoch 52/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7185 - accuracy: 0.7509 - val_loss: 0.6485 - val_accuracy: 0.6954\n",
            "Epoch 53/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.8035 - accuracy: 0.7046 - val_loss: 0.6453 - val_accuracy: 0.6897\n",
            "Epoch 54/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5915 - accuracy: 0.7277 - val_loss: 0.5790 - val_accuracy: 0.7011\n",
            "Epoch 55/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6105 - accuracy: 0.7267 - val_loss: 0.6104 - val_accuracy: 0.7299\n",
            "Epoch 56/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5898 - accuracy: 0.6705 - val_loss: 0.6339 - val_accuracy: 0.6494\n",
            "Epoch 57/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6389 - accuracy: 0.6384 - val_loss: 0.6734 - val_accuracy: 0.6034\n",
            "Epoch 58/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6875 - accuracy: 0.5154 - val_loss: 0.6343 - val_accuracy: 0.6724\n",
            "Epoch 59/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6435 - accuracy: 0.6271 - val_loss: 0.5902 - val_accuracy: 0.7356\n",
            "Epoch 60/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6088 - accuracy: 0.6789 - val_loss: 0.5949 - val_accuracy: 0.7069\n",
            "Epoch 61/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6140 - accuracy: 0.7244 - val_loss: 0.6582 - val_accuracy: 0.7011\n",
            "Epoch 62/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6985 - accuracy: 0.7073 - val_loss: 0.6677 - val_accuracy: 0.7011\n",
            "Epoch 63/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6781 - accuracy: 0.7024 - val_loss: 0.6687 - val_accuracy: 0.7011\n",
            "Epoch 64/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6322 - accuracy: 0.7247 - val_loss: 0.6566 - val_accuracy: 0.7069\n",
            "Epoch 65/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6578 - accuracy: 0.7115 - val_loss: 0.6040 - val_accuracy: 0.6954\n",
            "Epoch 66/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5970 - accuracy: 0.6881 - val_loss: 0.6360 - val_accuracy: 0.6897\n",
            "Epoch 67/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6287 - accuracy: 0.6433 - val_loss: 0.6223 - val_accuracy: 0.6954\n",
            "Epoch 68/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5971 - accuracy: 0.6920 - val_loss: 0.6002 - val_accuracy: 0.7184\n",
            "Epoch 69/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6033 - accuracy: 0.7141 - val_loss: 0.6629 - val_accuracy: 0.7069\n",
            "Epoch 70/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5990 - accuracy: 0.7444 - val_loss: 0.6715 - val_accuracy: 0.7069\n",
            "Epoch 71/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6263 - accuracy: 0.7336 - val_loss: 0.6831 - val_accuracy: 0.7069\n",
            "Epoch 72/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6659 - accuracy: 0.7018 - val_loss: 0.6764 - val_accuracy: 0.7069\n",
            "Epoch 73/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6533 - accuracy: 0.7169 - val_loss: 0.6704 - val_accuracy: 0.7011\n",
            "Epoch 74/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5724 - accuracy: 0.7399 - val_loss: 0.6639 - val_accuracy: 0.7011\n",
            "Epoch 75/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6008 - accuracy: 0.7487 - val_loss: 0.5898 - val_accuracy: 0.7299\n",
            "Epoch 76/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5675 - accuracy: 0.7082 - val_loss: 0.5938 - val_accuracy: 0.6954\n",
            "Epoch 77/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5630 - accuracy: 0.7256 - val_loss: 0.5907 - val_accuracy: 0.7184\n",
            "Epoch 78/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5801 - accuracy: 0.7286 - val_loss: 0.5854 - val_accuracy: 0.7184\n",
            "Epoch 79/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6078 - accuracy: 0.7321 - val_loss: 0.5815 - val_accuracy: 0.7299\n",
            "Epoch 80/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5711 - accuracy: 0.7147 - val_loss: 0.5804 - val_accuracy: 0.7069\n",
            "Epoch 81/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6369 - accuracy: 0.7012 - val_loss: 0.5840 - val_accuracy: 0.7069\n",
            "Epoch 82/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5869 - accuracy: 0.7170 - val_loss: 0.6119 - val_accuracy: 0.7069\n",
            "Epoch 83/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6025 - accuracy: 0.7535 - val_loss: 0.6506 - val_accuracy: 0.7069\n",
            "Epoch 84/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6372 - accuracy: 0.7311 - val_loss: 0.5846 - val_accuracy: 0.7069\n",
            "Epoch 85/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5489 - accuracy: 0.7471 - val_loss: 0.5805 - val_accuracy: 0.7184\n",
            "Epoch 86/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5714 - accuracy: 0.7440 - val_loss: 0.5823 - val_accuracy: 0.7184\n",
            "Epoch 87/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5658 - accuracy: 0.7364 - val_loss: 0.5816 - val_accuracy: 0.7241\n",
            "Epoch 88/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5849 - accuracy: 0.7777 - val_loss: 0.5789 - val_accuracy: 0.7184\n",
            "Epoch 89/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5588 - accuracy: 0.7345 - val_loss: 0.5791 - val_accuracy: 0.7126\n",
            "Epoch 90/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5881 - accuracy: 0.7397 - val_loss: 0.6484 - val_accuracy: 0.7069\n",
            "Epoch 91/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5722 - accuracy: 0.7277 - val_loss: 0.5872 - val_accuracy: 0.7069\n",
            "Epoch 92/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5470 - accuracy: 0.7308 - val_loss: 0.5889 - val_accuracy: 0.7069\n",
            "Epoch 93/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5927 - accuracy: 0.7259 - val_loss: 0.6011 - val_accuracy: 0.7069\n",
            "Epoch 94/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6090 - accuracy: 0.7227 - val_loss: 0.5786 - val_accuracy: 0.7069\n",
            "Epoch 95/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5649 - accuracy: 0.7469 - val_loss: 0.5750 - val_accuracy: 0.7069\n",
            "Epoch 96/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5790 - accuracy: 0.7269 - val_loss: 0.5742 - val_accuracy: 0.7069\n",
            "Epoch 97/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5668 - accuracy: 0.7484 - val_loss: 0.5756 - val_accuracy: 0.7069\n",
            "Epoch 98/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5519 - accuracy: 0.7313 - val_loss: 0.5753 - val_accuracy: 0.7011\n",
            "Epoch 99/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5415 - accuracy: 0.7299 - val_loss: 0.5739 - val_accuracy: 0.7069\n",
            "Epoch 100/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6235 - accuracy: 0.7231 - val_loss: 0.5753 - val_accuracy: 0.7011\n",
            "Epoch 101/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5864 - accuracy: 0.7602 - val_loss: 0.5800 - val_accuracy: 0.7069\n",
            "Epoch 102/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5562 - accuracy: 0.7488 - val_loss: 0.5855 - val_accuracy: 0.7069\n",
            "Epoch 103/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6087 - accuracy: 0.7315 - val_loss: 0.5859 - val_accuracy: 0.7069\n",
            "Epoch 104/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6463 - accuracy: 0.7354 - val_loss: 0.5814 - val_accuracy: 0.7069\n",
            "Epoch 105/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6617 - accuracy: 0.7195 - val_loss: 0.5774 - val_accuracy: 0.7011\n",
            "Epoch 106/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6447 - accuracy: 0.7150 - val_loss: 0.5726 - val_accuracy: 0.7184\n",
            "Epoch 107/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5647 - accuracy: 0.7311 - val_loss: 0.5732 - val_accuracy: 0.7299\n",
            "Epoch 108/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5747 - accuracy: 0.7477 - val_loss: 0.5731 - val_accuracy: 0.7356\n",
            "Epoch 109/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5796 - accuracy: 0.7353 - val_loss: 0.5723 - val_accuracy: 0.7356\n",
            "Epoch 110/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5533 - accuracy: 0.7482 - val_loss: 0.5720 - val_accuracy: 0.7184\n",
            "Epoch 111/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6438 - accuracy: 0.7257 - val_loss: 0.5736 - val_accuracy: 0.7069\n",
            "Epoch 112/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5586 - accuracy: 0.7431 - val_loss: 0.5780 - val_accuracy: 0.7069\n",
            "Epoch 113/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5815 - accuracy: 0.7265 - val_loss: 0.6436 - val_accuracy: 0.7184\n",
            "Epoch 114/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6100 - accuracy: 0.7318 - val_loss: 0.6507 - val_accuracy: 0.7126\n",
            "Epoch 115/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6471 - accuracy: 0.7350 - val_loss: 0.6625 - val_accuracy: 0.7069\n",
            "Epoch 116/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5666 - accuracy: 0.7450 - val_loss: 0.6544 - val_accuracy: 0.7069\n",
            "Epoch 117/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6310 - accuracy: 0.7345 - val_loss: 0.6462 - val_accuracy: 0.7184\n",
            "Epoch 118/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5788 - accuracy: 0.7193 - val_loss: 0.5841 - val_accuracy: 0.7126\n",
            "Epoch 119/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5599 - accuracy: 0.7358 - val_loss: 0.5756 - val_accuracy: 0.7356\n",
            "Epoch 120/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5616 - accuracy: 0.7707 - val_loss: 0.5757 - val_accuracy: 0.7299\n",
            "Epoch 121/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5744 - accuracy: 0.7348 - val_loss: 0.5756 - val_accuracy: 0.7299\n",
            "Epoch 122/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5566 - accuracy: 0.7710 - val_loss: 0.5773 - val_accuracy: 0.7241\n",
            "Epoch 123/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5496 - accuracy: 0.7384 - val_loss: 0.5832 - val_accuracy: 0.7184\n",
            "Epoch 124/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5974 - accuracy: 0.7191 - val_loss: 0.6065 - val_accuracy: 0.7126\n",
            "Epoch 125/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5908 - accuracy: 0.7362 - val_loss: 0.6566 - val_accuracy: 0.7069\n",
            "Epoch 126/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5895 - accuracy: 0.7382 - val_loss: 0.6667 - val_accuracy: 0.7069\n",
            "Epoch 127/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5297 - accuracy: 0.7583 - val_loss: 0.6619 - val_accuracy: 0.7069\n",
            "Epoch 128/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5743 - accuracy: 0.7258 - val_loss: 0.5866 - val_accuracy: 0.7126\n",
            "Epoch 129/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5620 - accuracy: 0.7536 - val_loss: 0.5807 - val_accuracy: 0.7126\n",
            "Epoch 130/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5607 - accuracy: 0.7564 - val_loss: 0.5803 - val_accuracy: 0.7126\n",
            "Epoch 131/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5470 - accuracy: 0.7798 - val_loss: 0.5787 - val_accuracy: 0.7184\n",
            "Epoch 132/300\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5990 - accuracy: 0.7341 - val_loss: 0.5780 - val_accuracy: 0.7184\n",
            "Epoch 133/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5317 - accuracy: 0.7401 - val_loss: 0.5781 - val_accuracy: 0.7184\n",
            "Epoch 134/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5718 - accuracy: 0.7329 - val_loss: 0.5807 - val_accuracy: 0.7126\n",
            "Epoch 135/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5535 - accuracy: 0.7344 - val_loss: 0.5873 - val_accuracy: 0.7069\n",
            "Epoch 136/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6039 - accuracy: 0.7366 - val_loss: 0.5946 - val_accuracy: 0.7069\n",
            "Epoch 137/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6227 - accuracy: 0.7451 - val_loss: 0.5768 - val_accuracy: 0.7184\n",
            "Epoch 138/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5609 - accuracy: 0.7600 - val_loss: 0.5746 - val_accuracy: 0.7184\n",
            "Epoch 139/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5516 - accuracy: 0.7556 - val_loss: 0.5744 - val_accuracy: 0.7184\n",
            "Epoch 140/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5353 - accuracy: 0.7467 - val_loss: 0.5752 - val_accuracy: 0.7184\n",
            "Epoch 141/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5850 - accuracy: 0.7202 - val_loss: 0.5790 - val_accuracy: 0.7126\n",
            "Epoch 142/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6096 - accuracy: 0.7388 - val_loss: 0.5812 - val_accuracy: 0.7126\n",
            "Epoch 143/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5811 - accuracy: 0.7269 - val_loss: 0.5768 - val_accuracy: 0.7184\n",
            "Epoch 144/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5547 - accuracy: 0.7327 - val_loss: 0.5750 - val_accuracy: 0.7126\n",
            "Epoch 145/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5670 - accuracy: 0.7476 - val_loss: 0.5747 - val_accuracy: 0.7126\n",
            "Epoch 146/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5420 - accuracy: 0.7570 - val_loss: 0.5743 - val_accuracy: 0.7069\n",
            "Epoch 147/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5727 - accuracy: 0.7501 - val_loss: 0.6494 - val_accuracy: 0.6207\n",
            "Epoch 148/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6952 - accuracy: 0.5629 - val_loss: 0.9700 - val_accuracy: 0.3218\n",
            "Epoch 149/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 1.0757 - accuracy: 0.3138 - val_loss: 0.9606 - val_accuracy: 0.3161\n",
            "Epoch 150/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.9212 - accuracy: 0.3214 - val_loss: 0.7576 - val_accuracy: 0.3506\n",
            "Epoch 151/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7122 - accuracy: 0.5099 - val_loss: 0.6554 - val_accuracy: 0.6379\n",
            "Epoch 152/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6268 - accuracy: 0.6956 - val_loss: 0.6210 - val_accuracy: 0.6839\n",
            "Epoch 153/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5853 - accuracy: 0.7221 - val_loss: 0.6170 - val_accuracy: 0.6839\n",
            "Epoch 154/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5539 - accuracy: 0.7269 - val_loss: 0.6232 - val_accuracy: 0.6839\n",
            "Epoch 155/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5738 - accuracy: 0.7199 - val_loss: 0.6274 - val_accuracy: 0.6839\n",
            "Epoch 156/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5836 - accuracy: 0.7087 - val_loss: 0.6139 - val_accuracy: 0.6839\n",
            "Epoch 157/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5806 - accuracy: 0.7161 - val_loss: 0.6299 - val_accuracy: 0.6839\n",
            "Epoch 158/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5902 - accuracy: 0.7259 - val_loss: 0.6394 - val_accuracy: 0.6839\n",
            "Epoch 159/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6000 - accuracy: 0.6973 - val_loss: 0.6345 - val_accuracy: 0.6839\n",
            "Epoch 160/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5818 - accuracy: 0.7251 - val_loss: 0.6246 - val_accuracy: 0.6839\n",
            "Epoch 161/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5770 - accuracy: 0.7256 - val_loss: 0.6170 - val_accuracy: 0.6897\n",
            "Epoch 162/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5627 - accuracy: 0.7250 - val_loss: 0.6125 - val_accuracy: 0.6897\n",
            "Epoch 163/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5430 - accuracy: 0.7312 - val_loss: 0.6116 - val_accuracy: 0.6839\n",
            "Epoch 164/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5601 - accuracy: 0.7082 - val_loss: 0.6119 - val_accuracy: 0.6839\n",
            "Epoch 165/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5568 - accuracy: 0.7070 - val_loss: 0.6132 - val_accuracy: 0.6839\n",
            "Epoch 166/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5698 - accuracy: 0.7088 - val_loss: 0.6149 - val_accuracy: 0.6839\n",
            "Epoch 167/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5609 - accuracy: 0.7244 - val_loss: 0.6106 - val_accuracy: 0.6839\n",
            "Epoch 168/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5634 - accuracy: 0.7004 - val_loss: 0.6051 - val_accuracy: 0.6839\n",
            "Epoch 169/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5700 - accuracy: 0.7072 - val_loss: 0.6018 - val_accuracy: 0.6897\n",
            "Epoch 170/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5526 - accuracy: 0.7144 - val_loss: 0.6002 - val_accuracy: 0.6897\n",
            "Epoch 171/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5691 - accuracy: 0.7292 - val_loss: 0.5989 - val_accuracy: 0.6897\n",
            "Epoch 172/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5567 - accuracy: 0.7269 - val_loss: 0.5973 - val_accuracy: 0.6897\n",
            "Epoch 173/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5461 - accuracy: 0.7349 - val_loss: 0.5961 - val_accuracy: 0.6897\n",
            "Epoch 174/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5500 - accuracy: 0.7219 - val_loss: 0.5947 - val_accuracy: 0.6897\n",
            "Epoch 175/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5467 - accuracy: 0.7194 - val_loss: 0.5927 - val_accuracy: 0.6897\n",
            "Epoch 176/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5528 - accuracy: 0.7201 - val_loss: 0.5911 - val_accuracy: 0.6897\n",
            "Epoch 177/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5586 - accuracy: 0.7108 - val_loss: 0.5898 - val_accuracy: 0.6897\n",
            "Epoch 178/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5576 - accuracy: 0.7122 - val_loss: 0.5878 - val_accuracy: 0.6954\n",
            "Epoch 179/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5473 - accuracy: 0.7335 - val_loss: 0.5872 - val_accuracy: 0.6954\n",
            "Epoch 180/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5443 - accuracy: 0.7392 - val_loss: 0.5861 - val_accuracy: 0.6954\n",
            "Epoch 181/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5479 - accuracy: 0.7315 - val_loss: 0.5856 - val_accuracy: 0.6897\n",
            "Epoch 182/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5753 - accuracy: 0.7497 - val_loss: 0.5866 - val_accuracy: 0.6897\n",
            "Epoch 183/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5907 - accuracy: 0.7121 - val_loss: 0.5871 - val_accuracy: 0.6897\n",
            "Epoch 184/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5340 - accuracy: 0.7394 - val_loss: 0.5876 - val_accuracy: 0.6897\n",
            "Epoch 185/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5128 - accuracy: 0.7434 - val_loss: 0.5880 - val_accuracy: 0.6897\n",
            "Epoch 186/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5332 - accuracy: 0.7306 - val_loss: 0.5838 - val_accuracy: 0.6954\n",
            "Epoch 187/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5476 - accuracy: 0.7377 - val_loss: 0.5842 - val_accuracy: 0.7011\n",
            "Epoch 188/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5531 - accuracy: 0.7363 - val_loss: 0.5863 - val_accuracy: 0.7069\n",
            "Epoch 189/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5370 - accuracy: 0.7391 - val_loss: 0.5863 - val_accuracy: 0.7069\n",
            "Epoch 190/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5544 - accuracy: 0.7574 - val_loss: 0.5844 - val_accuracy: 0.7011\n",
            "Epoch 191/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5454 - accuracy: 0.7392 - val_loss: 0.5819 - val_accuracy: 0.7011\n",
            "Epoch 192/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5458 - accuracy: 0.7561 - val_loss: 0.5811 - val_accuracy: 0.6954\n",
            "Epoch 193/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5407 - accuracy: 0.7345 - val_loss: 0.5812 - val_accuracy: 0.6954\n",
            "Epoch 194/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5321 - accuracy: 0.7294 - val_loss: 0.5814 - val_accuracy: 0.6954\n",
            "Epoch 195/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5387 - accuracy: 0.7195 - val_loss: 0.5824 - val_accuracy: 0.6954\n",
            "Epoch 196/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5747 - accuracy: 0.7151 - val_loss: 0.5823 - val_accuracy: 0.6954\n",
            "Epoch 197/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5337 - accuracy: 0.7260 - val_loss: 0.5831 - val_accuracy: 0.6954\n",
            "Epoch 198/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5531 - accuracy: 0.7198 - val_loss: 0.5794 - val_accuracy: 0.7011\n",
            "Epoch 199/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5450 - accuracy: 0.7433 - val_loss: 0.5851 - val_accuracy: 0.7011\n",
            "Epoch 200/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5334 - accuracy: 0.7765 - val_loss: 0.5899 - val_accuracy: 0.7011\n",
            "Epoch 201/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5588 - accuracy: 0.7559 - val_loss: 0.5895 - val_accuracy: 0.7011\n",
            "Epoch 202/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5669 - accuracy: 0.7544 - val_loss: 0.5832 - val_accuracy: 0.7011\n",
            "Epoch 203/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5478 - accuracy: 0.7537 - val_loss: 0.5776 - val_accuracy: 0.7069\n",
            "Epoch 204/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5539 - accuracy: 0.7304 - val_loss: 0.5747 - val_accuracy: 0.7069\n",
            "Epoch 205/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5501 - accuracy: 0.7292 - val_loss: 0.5743 - val_accuracy: 0.7011\n",
            "Epoch 206/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5715 - accuracy: 0.7215 - val_loss: 0.5744 - val_accuracy: 0.7011\n",
            "Epoch 207/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5479 - accuracy: 0.7242 - val_loss: 0.5745 - val_accuracy: 0.6954\n",
            "Epoch 208/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5193 - accuracy: 0.7553 - val_loss: 0.5744 - val_accuracy: 0.6954\n",
            "Epoch 209/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5437 - accuracy: 0.7342 - val_loss: 0.5738 - val_accuracy: 0.7011\n",
            "Epoch 210/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5355 - accuracy: 0.7328 - val_loss: 0.5741 - val_accuracy: 0.6954\n",
            "Epoch 211/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5348 - accuracy: 0.7388 - val_loss: 0.5739 - val_accuracy: 0.7011\n",
            "Epoch 212/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5235 - accuracy: 0.7455 - val_loss: 0.5734 - val_accuracy: 0.7011\n",
            "Epoch 213/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5459 - accuracy: 0.7265 - val_loss: 0.5732 - val_accuracy: 0.7011\n",
            "Epoch 214/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5480 - accuracy: 0.7381 - val_loss: 0.5735 - val_accuracy: 0.7011\n",
            "Epoch 215/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5427 - accuracy: 0.7260 - val_loss: 0.5735 - val_accuracy: 0.7011\n",
            "Epoch 216/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5363 - accuracy: 0.7354 - val_loss: 0.5734 - val_accuracy: 0.7069\n",
            "Epoch 217/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5354 - accuracy: 0.7300 - val_loss: 0.5733 - val_accuracy: 0.7069\n",
            "Epoch 218/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6098 - accuracy: 0.7408 - val_loss: 0.5731 - val_accuracy: 0.7069\n",
            "Epoch 219/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5414 - accuracy: 0.7387 - val_loss: 0.5733 - val_accuracy: 0.7069\n",
            "Epoch 220/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5213 - accuracy: 0.7504 - val_loss: 0.5734 - val_accuracy: 0.7069\n",
            "Epoch 221/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5353 - accuracy: 0.7391 - val_loss: 0.5730 - val_accuracy: 0.7126\n",
            "Epoch 222/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5650 - accuracy: 0.7473 - val_loss: 0.5733 - val_accuracy: 0.7126\n",
            "Epoch 223/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5327 - accuracy: 0.7439 - val_loss: 0.5735 - val_accuracy: 0.7126\n",
            "Epoch 224/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5777 - accuracy: 0.7392 - val_loss: 0.5729 - val_accuracy: 0.7126\n",
            "Epoch 225/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5364 - accuracy: 0.7472 - val_loss: 0.5724 - val_accuracy: 0.7126\n",
            "Epoch 226/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5747 - accuracy: 0.7548 - val_loss: 0.5721 - val_accuracy: 0.7126\n",
            "Epoch 227/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6148 - accuracy: 0.7243 - val_loss: 0.5692 - val_accuracy: 0.7126\n",
            "Epoch 228/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5533 - accuracy: 0.7394 - val_loss: 0.5691 - val_accuracy: 0.7126\n",
            "Epoch 229/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5378 - accuracy: 0.7436 - val_loss: 0.5680 - val_accuracy: 0.7184\n",
            "Epoch 230/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5458 - accuracy: 0.7389 - val_loss: 0.5706 - val_accuracy: 0.7126\n",
            "Epoch 231/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5377 - accuracy: 0.7469 - val_loss: 0.5719 - val_accuracy: 0.7126\n",
            "Epoch 232/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5473 - accuracy: 0.7383 - val_loss: 0.5730 - val_accuracy: 0.7126\n",
            "Epoch 233/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5343 - accuracy: 0.7676 - val_loss: 0.5731 - val_accuracy: 0.7126\n",
            "Epoch 234/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5497 - accuracy: 0.7521 - val_loss: 0.5719 - val_accuracy: 0.7126\n",
            "Epoch 235/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5369 - accuracy: 0.7451 - val_loss: 0.5729 - val_accuracy: 0.7126\n",
            "Epoch 236/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5475 - accuracy: 0.7462 - val_loss: 0.5827 - val_accuracy: 0.7126\n",
            "Epoch 237/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5543 - accuracy: 0.7463 - val_loss: 0.5857 - val_accuracy: 0.7126\n",
            "Epoch 238/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5538 - accuracy: 0.7582 - val_loss: 0.5830 - val_accuracy: 0.7011\n",
            "Epoch 239/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5522 - accuracy: 0.7236 - val_loss: 0.5799 - val_accuracy: 0.7069\n",
            "Epoch 240/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5464 - accuracy: 0.7421 - val_loss: 0.5781 - val_accuracy: 0.7069\n",
            "Epoch 241/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5514 - accuracy: 0.7026 - val_loss: 0.5775 - val_accuracy: 0.7126\n",
            "Epoch 242/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5487 - accuracy: 0.7505 - val_loss: 0.5784 - val_accuracy: 0.7069\n",
            "Epoch 243/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5341 - accuracy: 0.7256 - val_loss: 0.5788 - val_accuracy: 0.7011\n",
            "Epoch 244/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5354 - accuracy: 0.7421 - val_loss: 0.5789 - val_accuracy: 0.7011\n",
            "Epoch 245/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5378 - accuracy: 0.7352 - val_loss: 0.5784 - val_accuracy: 0.7069\n",
            "Epoch 246/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5568 - accuracy: 0.7299 - val_loss: 0.5858 - val_accuracy: 0.7184\n",
            "Epoch 247/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5783 - accuracy: 0.7282 - val_loss: 0.5869 - val_accuracy: 0.7356\n",
            "Epoch 248/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5615 - accuracy: 0.7664 - val_loss: 0.5811 - val_accuracy: 0.7241\n",
            "Epoch 249/300\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.5698 - accuracy: 0.7423 - val_loss: 0.5744 - val_accuracy: 0.7184\n",
            "Epoch 250/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5452 - accuracy: 0.7332 - val_loss: 0.5697 - val_accuracy: 0.6954\n",
            "Epoch 251/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5443 - accuracy: 0.7327 - val_loss: 0.5681 - val_accuracy: 0.6954\n",
            "Epoch 252/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5497 - accuracy: 0.7258 - val_loss: 0.5686 - val_accuracy: 0.6954\n",
            "Epoch 253/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5294 - accuracy: 0.7346 - val_loss: 0.5695 - val_accuracy: 0.6954\n",
            "Epoch 254/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5663 - accuracy: 0.7283 - val_loss: 0.5695 - val_accuracy: 0.6954\n",
            "Epoch 255/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5380 - accuracy: 0.7527 - val_loss: 0.5695 - val_accuracy: 0.6954\n",
            "Epoch 256/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5562 - accuracy: 0.7367 - val_loss: 0.5779 - val_accuracy: 0.7069\n",
            "Epoch 257/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5443 - accuracy: 0.7578 - val_loss: 0.5957 - val_accuracy: 0.7126\n",
            "Epoch 258/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5919 - accuracy: 0.7313 - val_loss: 0.6032 - val_accuracy: 0.7184\n",
            "Epoch 259/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5894 - accuracy: 0.7273 - val_loss: 0.6007 - val_accuracy: 0.7126\n",
            "Epoch 260/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5874 - accuracy: 0.7324 - val_loss: 0.5913 - val_accuracy: 0.7241\n",
            "Epoch 261/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5758 - accuracy: 0.7260 - val_loss: 0.5828 - val_accuracy: 0.7011\n",
            "Epoch 262/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5458 - accuracy: 0.7538 - val_loss: 0.5776 - val_accuracy: 0.7011\n",
            "Epoch 263/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5490 - accuracy: 0.7198 - val_loss: 0.5760 - val_accuracy: 0.6897\n",
            "Epoch 264/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5450 - accuracy: 0.7364 - val_loss: 0.5773 - val_accuracy: 0.6897\n",
            "Epoch 265/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5449 - accuracy: 0.7228 - val_loss: 0.5784 - val_accuracy: 0.6897\n",
            "Epoch 266/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5671 - accuracy: 0.7066 - val_loss: 0.5779 - val_accuracy: 0.6897\n",
            "Epoch 267/300\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.5410 - accuracy: 0.7415 - val_loss: 0.5777 - val_accuracy: 0.6897\n",
            "Epoch 268/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5396 - accuracy: 0.7274 - val_loss: 0.5757 - val_accuracy: 0.6954\n",
            "Epoch 269/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5505 - accuracy: 0.7370 - val_loss: 0.5753 - val_accuracy: 0.6954\n",
            "Epoch 270/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5387 - accuracy: 0.7377 - val_loss: 0.5750 - val_accuracy: 0.6954\n",
            "Epoch 271/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5284 - accuracy: 0.7317 - val_loss: 0.5749 - val_accuracy: 0.6954\n",
            "Epoch 272/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5801 - accuracy: 0.7313 - val_loss: 0.5753 - val_accuracy: 0.6954\n",
            "Epoch 273/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5480 - accuracy: 0.7263 - val_loss: 0.5760 - val_accuracy: 0.6954\n",
            "Epoch 274/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5793 - accuracy: 0.7295 - val_loss: 0.5757 - val_accuracy: 0.6954\n",
            "Epoch 275/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5533 - accuracy: 0.7182 - val_loss: 0.5747 - val_accuracy: 0.7011\n",
            "Epoch 276/300\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.5461 - accuracy: 0.7332 - val_loss: 0.5743 - val_accuracy: 0.7011\n",
            "Epoch 277/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5496 - accuracy: 0.7264 - val_loss: 0.5742 - val_accuracy: 0.7011\n",
            "Epoch 278/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5666 - accuracy: 0.7299 - val_loss: 0.5743 - val_accuracy: 0.7011\n",
            "Epoch 279/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5330 - accuracy: 0.7341 - val_loss: 0.5744 - val_accuracy: 0.7011\n",
            "Epoch 280/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5324 - accuracy: 0.7620 - val_loss: 0.5743 - val_accuracy: 0.7011\n",
            "Epoch 281/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5417 - accuracy: 0.7442 - val_loss: 0.5740 - val_accuracy: 0.7011\n",
            "Epoch 282/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5474 - accuracy: 0.7387 - val_loss: 0.5739 - val_accuracy: 0.7011\n",
            "Epoch 283/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5425 - accuracy: 0.7375 - val_loss: 0.5743 - val_accuracy: 0.7011\n",
            "Epoch 284/300\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5461 - accuracy: 0.7372 - val_loss: 0.5749 - val_accuracy: 0.7011\n",
            "Epoch 285/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5314 - accuracy: 0.7312 - val_loss: 0.5753 - val_accuracy: 0.7011\n",
            "Epoch 286/300\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5410 - accuracy: 0.7473 - val_loss: 0.5755 - val_accuracy: 0.7011\n",
            "Epoch 287/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5318 - accuracy: 0.7548 - val_loss: 0.5761 - val_accuracy: 0.7011\n",
            "Epoch 288/300\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5406 - accuracy: 0.7477 - val_loss: 0.5766 - val_accuracy: 0.7011\n",
            "Epoch 289/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5456 - accuracy: 0.7696 - val_loss: 0.5748 - val_accuracy: 0.7011\n",
            "Epoch 290/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5318 - accuracy: 0.7760 - val_loss: 0.5749 - val_accuracy: 0.7069\n",
            "Epoch 291/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5782 - accuracy: 0.7454 - val_loss: 0.5747 - val_accuracy: 0.7069\n",
            "Epoch 292/300\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5314 - accuracy: 0.7658 - val_loss: 0.5745 - val_accuracy: 0.7011\n",
            "Epoch 293/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5425 - accuracy: 0.7191 - val_loss: 0.5745 - val_accuracy: 0.7011\n",
            "Epoch 294/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5313 - accuracy: 0.7547 - val_loss: 0.5747 - val_accuracy: 0.7069\n",
            "Epoch 295/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5852 - accuracy: 0.7268 - val_loss: 0.5747 - val_accuracy: 0.7069\n",
            "Epoch 296/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5728 - accuracy: 0.7454 - val_loss: 0.5743 - val_accuracy: 0.6954\n",
            "Epoch 297/300\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5359 - accuracy: 0.7311 - val_loss: 0.5750 - val_accuracy: 0.7011\n",
            "Epoch 298/300\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5476 - accuracy: 0.7351 - val_loss: 0.5748 - val_accuracy: 0.7011\n",
            "Epoch 299/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5288 - accuracy: 0.7402 - val_loss: 0.5753 - val_accuracy: 0.7011\n",
            "Epoch 300/300\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5772 - accuracy: 0.7335 - val_loss: 0.5761 - val_accuracy: 0.7011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTm0-udWHm2v",
        "outputId": "d15ce53a-79a2-4d06-f88e-e460167b9d5f"
      },
      "source": [
        "Y_pred = model.predict(X_test)\r\n",
        "Y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.414464  ],\n",
              "       [0.18628152],\n",
              "       [0.27501032],\n",
              "       [0.14870289],\n",
              "       [0.32151884],\n",
              "       [0.27475244],\n",
              "       [0.3763953 ],\n",
              "       [0.31789485],\n",
              "       [0.37946135],\n",
              "       [0.42423946],\n",
              "       [0.3416221 ],\n",
              "       [0.17363867],\n",
              "       [0.35388285],\n",
              "       [0.31140465],\n",
              "       [0.4037895 ],\n",
              "       [0.22369118],\n",
              "       [0.5003775 ],\n",
              "       [0.3468601 ],\n",
              "       [0.45701045],\n",
              "       [0.37265205],\n",
              "       [0.1969454 ],\n",
              "       [0.20332865],\n",
              "       [0.19684379],\n",
              "       [0.40742743],\n",
              "       [0.28900906],\n",
              "       [0.28662556],\n",
              "       [0.08288765],\n",
              "       [0.20035724],\n",
              "       [0.31320226],\n",
              "       [0.27206075],\n",
              "       [0.15488175],\n",
              "       [0.5046218 ],\n",
              "       [0.4289813 ],\n",
              "       [0.35010564],\n",
              "       [0.15623252],\n",
              "       [0.07995799],\n",
              "       [0.05798441],\n",
              "       [0.12735906],\n",
              "       [0.33296335],\n",
              "       [0.25681257],\n",
              "       [0.22184929],\n",
              "       [0.3408099 ],\n",
              "       [0.1770569 ],\n",
              "       [0.18691719],\n",
              "       [0.33146426],\n",
              "       [0.2593281 ],\n",
              "       [0.27584493],\n",
              "       [0.25655454],\n",
              "       [0.17602028],\n",
              "       [0.41004246],\n",
              "       [0.34786046],\n",
              "       [0.3530947 ],\n",
              "       [0.12270734],\n",
              "       [0.45221752],\n",
              "       [0.20498985],\n",
              "       [0.03825262],\n",
              "       [0.45221275],\n",
              "       [0.51449424],\n",
              "       [0.23550475],\n",
              "       [0.13181315],\n",
              "       [0.32040128],\n",
              "       [0.31588644],\n",
              "       [0.18521301],\n",
              "       [0.17671305],\n",
              "       [0.21488813],\n",
              "       [0.30148554],\n",
              "       [0.39786744],\n",
              "       [0.26466957],\n",
              "       [0.19460912],\n",
              "       [0.29367036],\n",
              "       [0.3731682 ],\n",
              "       [0.22170904],\n",
              "       [0.10965618],\n",
              "       [0.3000867 ],\n",
              "       [0.09467313],\n",
              "       [0.2741231 ],\n",
              "       [0.3122309 ],\n",
              "       [0.25462338],\n",
              "       [0.18039176],\n",
              "       [0.33955118],\n",
              "       [0.35754514],\n",
              "       [0.14733712],\n",
              "       [0.10005926],\n",
              "       [0.19083084],\n",
              "       [0.        ],\n",
              "       [0.2551858 ],\n",
              "       [0.21635565],\n",
              "       [0.3635427 ],\n",
              "       [0.3101273 ],\n",
              "       [0.2357862 ],\n",
              "       [0.16249043],\n",
              "       [0.4600078 ],\n",
              "       [0.4204796 ],\n",
              "       [0.3787555 ],\n",
              "       [0.44065648],\n",
              "       [0.39736208],\n",
              "       [0.1782617 ],\n",
              "       [0.4184694 ],\n",
              "       [0.36858234],\n",
              "       [0.09826542],\n",
              "       [0.3352837 ],\n",
              "       [0.36133823],\n",
              "       [0.30597982],\n",
              "       [0.36503243],\n",
              "       [0.24084392],\n",
              "       [0.14776374],\n",
              "       [0.19370827],\n",
              "       [0.12769295],\n",
              "       [0.17320453],\n",
              "       [0.08533895],\n",
              "       [0.41606253],\n",
              "       [0.22170776],\n",
              "       [0.363368  ],\n",
              "       [0.3256122 ],\n",
              "       [0.2954781 ],\n",
              "       [0.04881898],\n",
              "       [0.37519902],\n",
              "       [0.4906191 ],\n",
              "       [0.28929257],\n",
              "       [0.39438045],\n",
              "       [0.24029644],\n",
              "       [0.2593327 ],\n",
              "       [0.31649438],\n",
              "       [0.25833374],\n",
              "       [0.14934434],\n",
              "       [0.48490876],\n",
              "       [0.25979134],\n",
              "       [0.3943619 ],\n",
              "       [0.21684524],\n",
              "       [0.28263807],\n",
              "       [0.43992317],\n",
              "       [0.39412773],\n",
              "       [0.29683638],\n",
              "       [0.27669764],\n",
              "       [0.24615821],\n",
              "       [0.38714302],\n",
              "       [0.14458509],\n",
              "       [0.2294849 ],\n",
              "       [0.15964091],\n",
              "       [0.46365452],\n",
              "       [0.18203866],\n",
              "       [0.15004869],\n",
              "       [0.20681478],\n",
              "       [0.26421756],\n",
              "       [0.17313603],\n",
              "       [0.12818883],\n",
              "       [0.24648406],\n",
              "       [0.42200214],\n",
              "       [0.19843262],\n",
              "       [0.14529154],\n",
              "       [0.3818515 ],\n",
              "       [0.30401337],\n",
              "       [0.38160047],\n",
              "       [0.22167552],\n",
              "       [0.24519514],\n",
              "       [0.48741198],\n",
              "       [0.42007515],\n",
              "       [0.36249572],\n",
              "       [0.06834997],\n",
              "       [0.11875364],\n",
              "       [0.3295529 ],\n",
              "       [0.2336683 ],\n",
              "       [0.1728111 ],\n",
              "       [0.21438399],\n",
              "       [0.17477041],\n",
              "       [0.4387424 ],\n",
              "       [0.42659315],\n",
              "       [0.1728295 ],\n",
              "       [0.30742225],\n",
              "       [0.3704382 ],\n",
              "       [0.32387826],\n",
              "       [0.34216526],\n",
              "       [0.37896752],\n",
              "       [0.2854582 ],\n",
              "       [0.2199812 ],\n",
              "       [0.07045901],\n",
              "       [0.22673666],\n",
              "       [0.4462629 ],\n",
              "       [0.33802956],\n",
              "       [0.34308326],\n",
              "       [0.13064557],\n",
              "       [0.40588424],\n",
              "       [0.38864735],\n",
              "       [0.29686847],\n",
              "       [0.28012773],\n",
              "       [0.18198615],\n",
              "       [0.3125562 ],\n",
              "       [0.28508314],\n",
              "       [0.2906165 ],\n",
              "       [0.17314135],\n",
              "       [0.24695288],\n",
              "       [0.14723615],\n",
              "       [0.15018125],\n",
              "       [0.4450536 ],\n",
              "       [0.25972608],\n",
              "       [0.10711625],\n",
              "       [0.42270297],\n",
              "       [0.36113018],\n",
              "       [0.32615897],\n",
              "       [0.16388331],\n",
              "       [0.06654251],\n",
              "       [0.18482327],\n",
              "       [0.08262895],\n",
              "       [0.35759884],\n",
              "       [0.48485503],\n",
              "       [0.14446348],\n",
              "       [0.4698395 ],\n",
              "       [0.36804998],\n",
              "       [0.22495128],\n",
              "       [0.51328564],\n",
              "       [0.4337833 ],\n",
              "       [0.3350165 ],\n",
              "       [0.26197103],\n",
              "       [0.20107698],\n",
              "       [0.42623472],\n",
              "       [0.27712587],\n",
              "       [0.2930284 ],\n",
              "       [0.18547623],\n",
              "       [0.3700734 ],\n",
              "       [0.21591108],\n",
              "       [0.44105902],\n",
              "       [0.38494343],\n",
              "       [0.25278097],\n",
              "       [0.38925976],\n",
              "       [0.27081016],\n",
              "       [0.07545826],\n",
              "       [0.07558095],\n",
              "       [0.2174749 ],\n",
              "       [0.25195375],\n",
              "       [0.13648069],\n",
              "       [0.26843426],\n",
              "       [0.3935802 ],\n",
              "       [0.13739093],\n",
              "       [0.4419207 ],\n",
              "       [0.262462  ],\n",
              "       [0.2871996 ],\n",
              "       [0.12408032],\n",
              "       [0.5126507 ],\n",
              "       [0.2803068 ],\n",
              "       [0.41186494],\n",
              "       [0.45424345],\n",
              "       [0.05580205],\n",
              "       [0.29324877],\n",
              "       [0.40951586],\n",
              "       [0.2943143 ],\n",
              "       [0.26400113],\n",
              "       [0.38926834],\n",
              "       [0.26341015],\n",
              "       [0.08331862],\n",
              "       [0.32381254],\n",
              "       [0.2111655 ],\n",
              "       [0.2701542 ],\n",
              "       [0.33774486],\n",
              "       [0.22062458],\n",
              "       [0.22571738],\n",
              "       [0.0781382 ],\n",
              "       [0.3175163 ],\n",
              "       [0.45466745],\n",
              "       [0.29217952],\n",
              "       [0.2553599 ],\n",
              "       [0.3813364 ],\n",
              "       [0.38627973],\n",
              "       [0.31775007],\n",
              "       [0.16498965],\n",
              "       [0.17431787],\n",
              "       [0.43625388],\n",
              "       [0.17291154],\n",
              "       [0.3151242 ],\n",
              "       [0.46799517],\n",
              "       [0.41959843],\n",
              "       [0.27175054],\n",
              "       [0.22214538],\n",
              "       [0.2674361 ],\n",
              "       [0.36242783],\n",
              "       [0.12258054],\n",
              "       [0.15539496],\n",
              "       [0.5173118 ],\n",
              "       [0.1080379 ],\n",
              "       [0.24374804],\n",
              "       [0.2809482 ],\n",
              "       [0.4918568 ],\n",
              "       [0.        ],\n",
              "       [0.38887817],\n",
              "       [0.4306356 ],\n",
              "       [0.48449832],\n",
              "       [0.23984043],\n",
              "       [0.10425237],\n",
              "       [0.4328562 ],\n",
              "       [0.36320624],\n",
              "       [0.32728314],\n",
              "       [0.08819383],\n",
              "       [0.16812488],\n",
              "       [0.2886905 ],\n",
              "       [0.4614669 ],\n",
              "       [0.13847405],\n",
              "       [0.49444145],\n",
              "       [0.2633114 ],\n",
              "       [0.22853234]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXbucqIJ2uX"
      },
      "source": [
        "Y_pred[Y_pred<0.5] = 0 \r\n",
        "Y_pred[Y_pred>=0.5] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ibRFRzj-JPN1",
        "outputId": "8dba59e4-ae4f-41ca-877f-940279fd1190"
      },
      "source": [
        "Y_pred = Y_pred.reshape(-1)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.hist(Y_pred,label='Predito')\r\n",
        "plt.hist(Y_test,alpha=0.5,label='De Fato')\r\n",
        "plt.legend\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}